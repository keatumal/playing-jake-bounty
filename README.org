* Bounty Submission: Emulating an Author's Style Using LLM

This is [[https://warpcast.com/anakvad][my]] submission for [[https://warpcast.com/~/conversations/0xfff0836147cf61b26b200bcdcc676d4be43ea867][Jake's bounty]]. The task is to generate a blog post using an LLM trained on posts from the [[https://www.blogofjake.com/][blogofjake.com]] blog.

You can find example generations in the [[file:data/result_examples/][data/result_examples]] directory.

My goal was to produce a ready-to-use model that could take a simple prompt like /"Write me an article about..."/ and generate the desired result.

I decided to try fine-tuning the =gpt-4o-mini-2024-07-18= model. This approach may be preferable to RAG or Few-Shot Prompting for the following reasons:

- Deep adaptation of the model for better replication of the author’s style.
- Consistent results.
- Improved contextual understanding.
- Flexibility in style emulation.
- Enhanced unique content generation.
- Lower memory and resource consumption.
- A ready-to-use model output without requiring examples.

This sounds promising, but I must admit this was my first attempt at fine-tuning, and the results were subpar. Unfortunately, English is not my native language, so I relied on ChatGPT for evaluation, which often flagged poor emulation of the author’s style.

** My Approach

1. The list of posts on the site’s page is loaded dynamically, so I used simple JavaScript code to extract the post URLs. This code is available in the [[file:extractURLs.js][extractURLs.js]] file.
2. I then used Beautiful Soup to scrape the articles and convert them into Markdown format. This resulted in =248= posts stored in the [[file:data/blog_posts_md/][data/blog_posts_md]] directory. The script for this is [[file:fetch_posts.py][fetch_posts.py]].
3. Next, I used =gpt-4o-mini= to generate user-role prompts for the dataset to be used in fine-tuning. This task is handled by the [[file:generate_prompts.py][generate_prompts.py]] script.
4. Then I created training and testing datasets using the [[file:create_dataset.py][create_dataset.py]] script.
5. The next step was fine-tuning through the platform.openai.com interface.
6. Finally, the [[file:generate_examples.py][generate_examples.py]] script took =N= random prompts and generated articles using the new model.

** First Fine-Tuning Attempt

I selected 100 random posts for the training dataset and 75 posts for testing. Settings:

- Batch size: 1 (automatically selected)
- LR multiplier: 1.8 (automatically selected)
- Epochs: 3 (set manually)

OpenAI returned errors twice, but the process completed on the third attempt in 9 minutes. Screenshot of the results:

#+caption: [[file:images/fine_tuning_attempt_1.png][First Fine-Tuning Attempt]]
#+name: fig:fine-tuning-atttempt-1
[[file:images/fine_tuning_attempt_1.png]]

Examples of generated content can be found in the [[file:data/result_examples/attempt_1/][data/result_examples/attempt_1]] directory.

** Second Attempt

I noticed the model often inserted random links in the results. Therefore, I decided to convert all Markdown files into plain text format without markup. I replaced links with text or the string =[URL]= if they contained a URL. This task is handled by the [[file:md_to_txt.py][md_to_txt.py]] script.

This time, I used all articles, allocating 223 for training and 25 for testing. I also changed the settings:

- Batch size: 1
- LR multiplier: 0.05
- Epochs: 5

The fine-tuning process lasted 20 minutes and succeeded on the first attempt. Results screenshot:

#+caption: [[file:images/fine_tuning_attempt_1.png][Second Fine-Tuning Attempt]]
#+name: fig:fine-tuning-atttempt-2
[[file:images/fine_tuning_attempt_2.png]]

Generated examples are available in the [[file:data/result_examples/attempt_2/][data/result_examples/attempt_2]] directory.

** Experimenting with Temperature & Top P

I took the second model and used a more advanced article-specific prompt to generate the article via playground. I set the temperature to 0.7 and "Top P" to 0.85. The result is in the file [[file:data/result_examples/attempt_2_playground/10x-technologies.txt][data/result_examples/attempt_2_playground/10x-technologies.txt]].

I also tried a predefined prompt from a file, with a temperature of 0.6 and top p of 0.85. The result is in the file [[file:data/result_examples/attempt_2_playground/the-zoom-boom.txt][data/result_examples/attempt_2_playground/the-zoom-boom.txt]].

** Conclusions

The results are unsatisfactory. However, I believe that with proper knowledge and expertise, this approach could be the best solution. My understanding of this field is too superficial to achieve the desired outcome.

** Materials used

This project utilizes articles obtained from [[https://www.blogofjake.com/][blogofjake.com]]. All rights to the materials belong to their authors.
