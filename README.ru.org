* Заявка на баунти по имитации стиля автора через LLM

Это [[https://warpcast.com/anakvad][моя]] заявка на [[https://warpcast.com/~/conversations/0xfff0836147cf61b26b200bcdcc676d4be43ea867][баунти от Джейка]]. Необходимо сгенерировать пост для блога через LLM, натренированную на постах с блога [[https://www.blogofjake.com/][blogofjake.com]].

Примеры генерации можно найти в директории [[file:data/result_examples/][data/result_examples]].

Моей целью было получить готовую модель, которой можно дать простой промпт вида /"Напиши мне статью на тему..."/ и получить нужный результат.

Я решил попробовать решение через файн-тюнинг для модели =gpt-4o-mini-2024-07-18=. Оно может быть предпочтительней перед RAG или Few-Shot Prompting по следующим причинам:

- Глубокая адаптация модели, для лучшей передачи стиля автора.
- Стабильность результатов.
- Более лучшее контекстуальное понимание.
- Гибкость в имитации стилей.
- Улучшенная генерация уникального контента.
- Меньшее потребление памяти и ресурсов.
- На выходе будет готовая модель, которую можно использовать не предоставляя примеры.

Звучит очень хорошо. Но, признаюсь честно, это мой первый опыт по файн-тюнингу и похоже я получил плохие результаты. К сожалению, английский не мой родной язык, и для оценки я использовал ChatGPT, который часто указывал на плохую имитацию стиля автора.

** Мой подход

1. На странице сайта со всеми постами их список подгружается динамически, поэтому я для простоты использовал простой JS код для извлечения URL постов. Его можно найти в файле [[file:extractURLs.js][extractURLs.js]].
2. Дальше я через Beautiful Soup спарсил статьи и конвертировал их в Markdown. Получилось =248= постов в директории [[file:data/blog_posts_md/][data/blog_posts_md]]. Этим занимается скрипт [[file:fetch_posts.py][fetch_posts.py]].
3. Следующим шагом я через =gpt-4o-mini= сгенерировал промпты роли пользователя для датасета, который будет использоваться в файн-тюнинге. Эту задачу выполняет скрипт [[file:generate_prompts.py][generate_prompts.py]].
4. Потом я через скрипт [[file:create_dataset.py][create_dataset.py]] создал датасеты для тренировки и тестирования модели.
5. Следующим шагом был файн-тюнинг через интерфейс platform.openai.com.
6. Потом скрипт [[file:generate_examples.py][generate_examples.py]] брал =N= случайных промптов и генерировал по ним статьи через новую модель.

** Первая попытка файн-тюнинга

Я взял 100 случайных постов для тренировочного датасета, и 75 постов для тестового. Настройки:

- Batch size: 1 (выбрано автоматически)
- LR multiplier: 1.8 (выбрано автоматически)
- Epochs: 3 (установлено мною)

Два раза OpenAI выдавал ошибку, но на третью попытку процесс завершился за 9 минут. Скриншот результатов:

#+caption: Первая попытка файн-тюнинга
#+name: fig:fine-tuning-atttempt-1
[[file:images/fine_tuning_attempt_1.png]]

Примеры генерации можно найти в директории [[file:data/result_examples/attempt_1/][data/result_examples/attempt_1]].

** Вторая попытка

Я заметил, что в результатах модель часто вставляет рандомные ссылки. Поэтому я решил преобразовать все Markdown-файлы в обычный текстовый формат без разметки. Ссылки я заменил текстом, или строкой =[URL]=, если они содержали URL. Этим занимается скрипт [[file:md_to_txt.py][md_to_txt.py]].

Также на этот раз я решил использовать все статьи, выделив 223 статьи для тренировки, и 25 для теста. Также я поменял настройки:

- Batch size: 1
- LR multiplier: 0.05
- Epochs: 5

Файн-тюнинг длился 20 минут, на этот раз с первой попытки. Результаты на скриншоте:

#+caption: Вторая попытка файн-тюнинга
#+name: fig:fine-tuning-atttempt-2
[[file:images/fine_tuning_attempt_2.png]]

Примеры генерации в директории [[file:data/result_examples/attempt_2/][data/result_examples/attempt_2]].

** Попытка настройки temperature & top p

Я взял вторую модель и использовал более расширенный промпт, заточенный под статью, для генерации статьи через playground. Temperature установил в 0.7, а "Top P" в 0.85. Результат в файле [[file:data/result_examples/attempt_2_playground/10x-technologies.txt][data/result_examples/attempt_2_playground/10x-technologies.txt]].

Потом я попробовал готовый промпт из файла, с temperature = 0.6 и top p = 0.85. Результат в файле [[file:data/result_examples/attempt_2_playground/the-zoom-boom.txt][data/result_examples/attempt_2_playground/the-zoom-boom.txt]].

** Выводы

Результаты неудовлетворительны. Но всё же я считаю, что при должных знаниях и опыте этот подход может быть лучшим решением. Мои знания в этой области крайне поверхностны, чтобы добиться желаемого результата.

** Используемые материалы

В этом проекте использованы статьи, полученные с сайта [[https://www.blogofjake.com/][blogofjake.com]]. Все права на материалы принадлежат их авторам.
